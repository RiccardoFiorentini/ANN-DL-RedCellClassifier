# -*- coding: utf-8 -*-
"""Homework_1_densenet_lion_def

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R0wM1AAV5KvoPgLrGiXhooHI02I5Y9Wp

## üåê Connect Colab to Google Drive
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive

drive.mount('/gdrive')
# %cd /gdrive/My Drive/Colab Notebooks/Homework 1_lst
# %ls

"""## ‚öôÔ∏è Import Libraries"""

# Commented out IPython magic to ensure Python compatibility.
# Set seed for reproducibility
seed = 42

# Import necessary libraries
import os

!pip install keras-cv
import keras_cv

# Set environment variables before importing modules
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['PYTHONHASHSEED'] = str(seed)
os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'

# Suppress warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=Warning)

# Import necessary modules
import logging
import random
import numpy as np

# Set seeds for random number generators in NumPy and Python
np.random.seed(seed)
random.seed(seed)

# Import TensorFlow and Keras
import tensorflow as tf
from tensorflow import keras as tfk
from tensorflow.keras import layers as tfkl

# Set seed for TensorFlow
tf.random.set_seed(seed)
tf.compat.v1.set_random_seed(seed)

# Reduce TensorFlow verbosity
tf.autograph.set_verbosity(0)
tf.get_logger().setLevel(logging.ERROR)
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

# Print TensorFlow version
print(tf.__version__)

# Import other libraries
import requests
from io import BytesIO
import cv2
from PIL import Image
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns

# Configure plot display settings
sns.set(font_scale=1.4)
sns.set_style('white')
plt.rc('font', size=14)

# %matplotlib inline

"""## ‚è≥ Load the Data"""

data = np.load('training_augmented_big.npz')
X_train = data['X_train']
y_train = data['y_train']
print(X_train.shape, y_train.shape)

data = np.load('validation_test.npz')
X_val = data['X_val']
y_val = data['y_val']
print(X_val.shape, y_val.shape)
X_test = data['X_test']
y_test = data['y_test']
print(X_test.shape, y_test.shape)

"""## ‚öì DenseNet for Transfer Learning"""

from tensorflow.keras.applications import DenseNet121

# Initialise DenseNet model with pretrained weights, for transfer learning
densenet = DenseNet121(
    input_shape=(96, 96, 3),
    include_top=False,
    weights='imagenet',
    pooling='avg'
)

# Freeze all layers to use it solely as a feature extractor
densenet.trainable = False

# Define input layer with shape matching the input images
inputs = tfk.Input(shape=(96, 96, 3), name='input_layer')

# Apply data augmentation for training robustness
augmentation = tf.keras.Sequential([
    tfkl.RandomFlip("horizontal_vertical"),
    tfkl.RandomTranslation(0.25, 0.25),
    tfkl.RandomZoom(0.4),
    tfkl.RandomRotation(0.6),
    tfkl.RandomContrast(0.3),
    tfkl.RandomBrightness(0.4)
], name='preprocessing')

x = augmentation(inputs)

# Rescaling
x = tfkl.Rescaling(1./255)(x)

# Pass augmented inputs through the denseNet feature extractor
x = densenet(x)

# Add a dropout layer for regularisation
x = tfkl.Dropout(0.4, name='dropout')(x)

# Add final Dense layer for classification with softmax activation
outputs = tfkl.Dense(y_train.shape[-1], activation='softmax', name='dense')(x)

# Define the complete model linking input and output
inception_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

# Lion configuratin:
lion_optimizer = tfk.optimizers.Lion(
    learning_rate = 0.001,
    weight_decay = 0.01,
)

# Compile with Lion
inception_model.compile(
    optimizer=lion_optimizer,
    loss=tfk.losses.CategoricalCrossentropy(),
    metrics=['accuracy']
)

# Display a summary of the model architecture
densenet.summary(expand_nested=False)

# Display model architecture with layer shapes and trainable parameters
tfk.utils.plot_model(inception_model, expand_nested=True, show_trainable=True,
                     show_shapes=True, dpi=70)

"""## üòì Training"""

# Against overfitting
early_stopping_inception = tfk.callbacks.EarlyStopping(
    monitor = 'val_accuracy',
    mode = 'max',
    patience = 10,
    restore_best_weights=True
)

# Store the callback in a list
callbacks = [early_stopping_inception]

# Train the model
inception_history = inception_model.fit(
    x = X_train,
    y = y_train,
    batch_size = 128,
    epochs = 300,
    validation_data = (X_val, y_val),
    callbacks = callbacks,
    shuffle = True
).history

# Calculate and print the best validation accuracy achieved
final_val_accuracy = round(max(inception_history['val_accuracy']) * 100, 2)
print(f'Final validation accuracy: {final_val_accuracy}%')

# Save the trained model to a file, including final accuracy in the filename
model_filename = 'densenet_base.keras'
inception_model.save(model_filename)

# Free memory by deleting the model instance
del inception_model

"""## ‚è∞ First fine tuning"""

# Re-load the model
inception_model = tfk.models.load_model('densenet_base.keras')

print(len(inception_model.get_layer('densenet121').layers))

for layer in inception_model.layers:
    print(layer.name, type(layer).__name__)

# Set the model layers as trainable
inception_model.get_layer('densenet121').trainable = True

# Set all layers as non-trainable
for layer in inception_model.get_layer('densenet121').layers:
    layer.trainable = False

# Enable training only for Conv2D and DepthwiseConv2D layers
for i, layer in enumerate(inception_model.get_layer('densenet121').layers):
    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):
        layer.trainable = True
        print(i, layer.name, type(layer).__name__, layer.trainable)

# Set the number of layers to freeze (beginning)
N = 350

# Set the first N layers as non-trainable
for i, layer in enumerate(inception_model.get_layer('densenet121').layers[:N]):
    layer.trainable = False

# Print layer indices, names, and trainability status
for i, layer in enumerate(inception_model.get_layer('densenet121').layers):
    print(i, layer.name, layer.trainable)

# Compile
inception_model.compile(
    optimizer=tfk.optimizers.Nadam(5e-4),
    loss=tfk.losses.CategoricalCrossentropy(),
    metrics=['accuracy']
)

# The "for" iteration is paramount to face the colab limited resources and gave us the oportunity
# to split the training in different phases saving the partial results iteratively
for i in range(3):
  # Fine-tune the model
  inception_history = inception_model.fit(
      x = X_train,
      y = y_train,
      batch_size = 64,
      epochs = 30,
      validation_data = (X_val, y_val),
      callbacks=[
          tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=5, restore_best_weights=True),
          tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)
      ],
      shuffle = True
  ).history

  # Calculate and print the final validation accuracy
  final_val_accuracy = round(max(inception_history['val_accuracy'])* 100, 2)
  print(f'Final validation accuracy: {final_val_accuracy}%')

  # Save the trained model to a file with the accuracy included in the filename
  model_filename = 'inception_model_ft1.keras'
  inception_model.save(model_filename)

# Delete the model to free up resources
del inception_model

"""## ‚õ≥ Second fine tuning"""

# Re-load the model
inception_model = tfk.models.load_model('inception_model_ft1.keras')

for layer in inception_model.layers:
    print(layer.name, type(layer).__name__)

# Set the model layers as trainable
inception_model.get_layer('densenet121').trainable = True

# Set all layers as non-trainable
for layer in inception_model.get_layer('densenet121').layers:
    layer.trainable = False

# Enable training only for Conv2D and DepthwiseConv2D layers
for i, layer in enumerate(inception_model.get_layer('densenet121').layers):
    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):
        layer.trainable = True
        print(i, layer.name, type(layer).__name__, layer.trainable)

# Set the number of layers to freeze
N = 200

# Set the first N layers as non-trainable
for i, layer in enumerate(inception_model.get_layer('densenet121').layers[:N]):
    layer.trainable = False

# Print layer indices, names, and trainability status
for i, layer in enumerate(inception_model.get_layer('densenet121').layers):
    print(i, layer.name, layer.trainable)

# Compile
inception_model.compile(
    optimizer=tfk.optimizers.Nadam(2e-4),
    loss=tfk.losses.CategoricalCrossentropy(),
    metrics=['accuracy']
)

for i in range(3):
  # Fine-tune the model
  inception_history = inception_model.fit(
      x = X_train,
      y = y_train,
      batch_size = 128,
      epochs = 30,
      validation_data = (X_val, y_val),
      callbacks=[
          tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=5, restore_best_weights=True),
          tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)
      ],
      shuffle = True
  ).history

  # Calculate and print the final validation accuracy
  final_val_accuracy = round(max(inception_history['val_accuracy'])* 100, 2)
  print(f'Final validation accuracy: {final_val_accuracy}%')

  # Save the trained model to a file with the accuracy included in the filename
  model_filename = 'inception_model_ft2.keras'
  inception_model.save(model_filename)

# Delete the model to free up resources
del inception_model

"""## ü•µ Third fine tuning

"""

# Re-load the model
inception_model = tfk.models.load_model('inception_model_ft2.keras')

for layer in inception_model.layers:
    print(layer.name, type(layer).__name__)

# Set the model layers as trainable
inception_model.get_layer('densenet121').trainable = True

# Set all layers as non-trainable
for layer in inception_model.get_layer('densenet121').layers:
    layer.trainable = False

# Enable training only for Conv2D and DepthwiseConv2D layers
for i, layer in enumerate(inception_model.get_layer('densenet121').layers):
    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):
        layer.trainable = True
        print(i, layer.name, type(layer).__name__, layer.trainable)

# Set the number of layers to freeze
N = 50

# Set the first N layers as non-trainable
for i, layer in enumerate(inception_model.get_layer('densenet121').layers[:N]):
    layer.trainable = False

# Print layer indices, names, and trainability status
for i, layer in enumerate(inception_model.get_layer('densenet121').layers):
    print(i, layer.name, layer.trainable)

# Compile
inception_model.compile(
    optimizer=tfk.optimizers.Nadam(1e-4),
    loss=tfk.losses.CategoricalCrossentropy(),
    metrics=['accuracy']
)

for i in range(3):
  # Fine-tune the model
  inception_history = inception_model.fit(
      x = X_train,
      y = y_train,
      batch_size = 256,
      epochs = 20,
      validation_data = (X_val, y_val),
      callbacks=[
          tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=5, restore_best_weights=True),
          tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-8)
      ],
      shuffle = True
  ).history

  # Calculate and print the final validation accuracy
  final_val_accuracy = round(max(inception_history['val_accuracy'])* 100, 2)
  print(f'Final validation accuracy: {final_val_accuracy}%')

  # Save the trained model to a file with the accuracy included in the filename
  model_filename = 'densenet_model_ft3.keras'
  inception_model.save(model_filename)

# Delete the model to free up resources
del inception_model

"""## ü¶Ç: Fourth fine tuning"""

# Re-load the model
inception_model = tfk.models.load_model('densenet_model_ft3.keras')

for layer in inception_model.layers:
    print(layer.name, type(layer).__name__)

# Set the model layers as trainable
inception_model.get_layer('densenet121').trainable = True

# Set all layers as non-trainable
for layer in inception_model.get_layer('densenet121').layers:
    layer.trainable = False

# Enable training only for Conv2D and DepthwiseConv2D layers
for i, layer in enumerate(inception_model.get_layer('densenet121').layers):
    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):
        layer.trainable = True
        print(i, layer.name, type(layer).__name__, layer.trainable)

# Set the number of layers to freeze
N = 10

# Set the first N layers as non-trainable
for i, layer in enumerate(inception_model.get_layer('densenet121').layers[:N]):
    layer.trainable = False

# Print layer indices, names, and trainability status
for i, layer in enumerate(inception_model.get_layer('densenet121').layers):
    print(i, layer.name, layer.trainable)

# Compile
inception_model.compile(
    optimizer=tfk.optimizers.Nadam(8e-5),
    loss=tfk.losses.CategoricalCrossentropy(),
    metrics=['accuracy']
)

for i in range(3):
  # Fine-tune the model
  inception_history = inception_model.fit(
      x = X_train,
      y = y_train,
      batch_size = 256,
      epochs = 20,
      validation_data = (X_val, y_val),
      callbacks=[
          tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=5, restore_best_weights=True),
          tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-8)
      ],
      shuffle = True
  ).history

  # Calculate and print the final validation accuracy
  final_val_accuracy = round(max(inception_history['val_accuracy'])* 100, 2)
  print(f'Final validation accuracy: {final_val_accuracy}%')

  # Save the trained model to a file with the accuracy included in the filename
  model_filename = 'densenet_model_ft4.keras'
  inception_model.save(model_filename)

# Delete the model to free up resources
del inception_model

"""## üíí Fifth fine tuning (with randAugment only)"""

del X_train
del y_train
data = np.load('training_augmented_big_randaugment.npz')
X_train_2 = data['X_train']
y_train_2 = data['y_train']
print(X_train_2.shape, y_train_2.shape)

# Re-load the model
inception_model = tfk.models.load_model('densenet_model_ft4.keras')

for layer in inception_model.layers:
    print(layer.name, type(layer).__name__)

# Set the model layers as trainable
inception_model.get_layer('densenet121').trainable = True

# Set all layers as non-trainable
for layer in inception_model.get_layer('densenet121').layers:
    layer.trainable = False

# Enable training only for Conv2D and DepthwiseConv2D layers
for i, layer in enumerate(inception_model.get_layer('densenet121').layers):
    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.DepthwiseConv2D):
        layer.trainable = True
        print(i, layer.name, type(layer).__name__, layer.trainable)

# Set the number of layers to freeze
N = 20

# Set the first N layers as non-trainable
for i, layer in enumerate(inception_model.get_layer('densenet121').layers[:N]):
    layer.trainable = False

# Print layer indices, names, and trainability status
for i, layer in enumerate(inception_model.get_layer('densenet121').layers):
    print(i, layer.name, layer.trainable)

# Compile
inception_model.compile(
    optimizer=tfk.optimizers.Nadam(8e-5),
    loss=tfk.losses.CategoricalCrossentropy(),
    metrics=['accuracy']
)

for i in range(3):
  # Fine-tune the model
  inception_history = inception_model.fit(
      x = X_train_2,
      y = y_train_2,
      batch_size = 256,
      epochs = 30,
      validation_data = (X_val, y_val),
      callbacks=[
          tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=8, restore_best_weights=True),
          tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-8)
      ],
      shuffle = True
  ).history

  # Calculate and print the final validation accuracy
  final_val_accuracy = round(max(inception_history['val_accuracy'])* 100, 2)
  print(f'Final validation accuracy: {final_val_accuracy}%')

  # Save the trained model to a file with the accuracy included in the filename
  model_filename = 'densenet_model_def.keras'
  inception_model.save(model_filename)

# Delete the model to free up resources
del inception_model

"""## ‚ôü Evaluate InceptionResNetV2 Accuracy"""

# Load the fine-tuned model for prediction on external images
inception_model = tfk.models.load_model('densenet_model_def.keras')

# Predict class probabilities and get predicted classes
test_predictions = inception_model.predict(X_test, verbose=0)
test_predictions = np.argmax(test_predictions, axis=-1)

# Extract ground truth classes
test_gt = np.argmax(y_test, axis=-1)

# Calculate and display test set accuracy
test_accuracy = accuracy_score(test_gt, test_predictions)
print(f'Accuracy score over the test set: {round(test_accuracy, 4)}')

"""## üé∑: Test Time Augmentation"""

# Load the fine-tuned model for prediction on external images
inception_model = tfk.models.load_model('densenet_model_def.keras')

model = inception_model

model.save('weights.keras')
del model
del inception_model

# Commented out IPython magic to ensure Python compatibility.
# %%writefile model.py
# import numpy as np
# 
# import tensorflow as tf
# from tensorflow import keras as tfk
# from tensorflow.keras import layers as tfkl
# 
# from tensorflow.keras.layers import Layer
# import tensorflow.keras.backend as K
# 
# import keras_cv as kcv
# 
# import random
# 
# augmentation = tf.keras.Sequential([
#     tfkl.RandomFlip("horizontal_vertical"),
#     tfkl.RandomTranslation(0.25, 0.25),
#     tfkl.RandomZoom(0.2),
#     tfkl.RandomRotation(0.4),
#     tfkl.RandomContrast(0.2),
#     tfkl.RandomBrightness(0.2)
# ], name='preprocessing')
# 
# class Model:
#     def __init__(self):
#         self.neural_network = tfk.models.load_model('weights.keras')
# 
#     def predict(self, X):
#         preds = self.neural_network.predict(X)
#         for i in range(30):
#             preds = preds + self.neural_network.predict(augmentation(X))
#         if len(preds.shape) == 2:
#             preds = np.argmax(preds, axis=1)
#         return preds

from datetime import datetime
filename = f'submission_{datetime.now().strftime("%y%m%d_%H%M%S")}.zip'

# Add files to the zip command if needed
!zip {filename} model.py weights.keras

from google.colab import files
files.download(filename)